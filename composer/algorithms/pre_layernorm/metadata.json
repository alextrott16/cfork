{
    "pre_layernorm": {
        "name": "Pre-LayerNorm",
        "class_name": "PreLayerNorm",
        "functional": "apply_pre_layernorm",
        "tldr": "Converts the default Post-LN architecture into the Pre-LN (optionally, NormFormer) architecture.",
        "attribution": "(Xiong et al, 2020), see also (Shleifer et al, 2021)",
        "link": "http://arxiv.org/abs/2002.04745",
        "domains": [
            "nlp"
        ],
        "summary": "The Pre-LN configuration makes BERT training more stable, allowing higher learning rates. Additionally, the NormFormer variant improves convergence.",
        "use": "Transformers"
    }
}